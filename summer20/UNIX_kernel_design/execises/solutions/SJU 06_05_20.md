# SJU 06/05/20

## ZAD 1
1. Pojęcia:
    * Systemy SMP (Symmetric MultiProcessing) - jedna kopia SO w pamięci, ale może działać na dowolnym procesorze. Model ten równoważy pamięć i procesy oraz eliminuje problem wąskiego gardła procesu nadrzędnego, które występuje przy innym podejściu, ale pojawiają się inne problemy - synchronizacja.(wielka blokada jądra, zwiększamy ziarnistość, tablice, zawieszanie się systemu) ![](https://i.imgur.com/b9FIn0P.png)

    * Fałszywe współdzielenie (false sharing) - False sharing occurs when different threads have data that is not shared in the program, but this data gets mapped to a cache line that is shared. ![](https://i.imgur.com/HJeaBD8.png)


    * Gra w ping-pong (cache ping-pong) - Cache line ping-ponging is the effect where a cache line is transferred between multiple CPUs (or cores) in rapid succession. This can be cause by either false or true sharing. Essentially if multiple CPUs are trying to read and write data in the same cache line then that cache line might have to be transferred between the two threads in rapid succession, and this can cause a significant performance degradation. Przykład z SpinLockami

### Czemu koszt operacji atomowych (np. compare-and-swap) jest wysoki w systemach SMP?
Instrukcja TSL (Test and Set Lock) bez blokady magistrali.
![](https://i.imgur.com/Pk7a9pj.png)
Ponieważ musimy blokować magistralę.

### Czemu w danej linii pamięci podręcznej powinna znajdować się tylko jedna blokada wirująca?
Ponieważ procesor kopiuje linie pamięci do pamięci podręcznej na czas trzymania blokady, podobnie procesy nie trzymjace blokady. I przy zwalnianiu blokady zostaje wysłane info, że ta pamięć podręczna jest nieaktualna i mają ją zaktualizować i w przypadku wielu blokad w tej samej linii można stworzyć duże obciążenie wysyłając wszystkim informacje (nawet z tymi, którezy nie są z nami związani, a także chyba pojawia się problem, że pamięć podręczna procesów trzymających blokade może się różnić i nie wiemy co się stanie)

### W procedurze [mutex_vector_enter](http://bxr.su/NetBSD/sys/kern/kern_mutex.c#mutex_vector_enter) zidentyfikuj kod odpowiadający za opisany w książce algorytm exponential backoff i wyjaśnij jego działanie
*    algorytm exponential backoff - zamiast ciągłego odpytywania o zmiane stanu zwiększamy opóźnienie zapytania wykładniczego do określonej wartości, przez co oszczędzamy na zasobach - pamięci i procesorze. W przypadku muteksa to akurat pamięć.

[SPINLOCK_BACKOFF](http://bxr.su/NetBSD/sys/sys/lock.h#94)
```C
94#define SPINLOCK_BACKOFF(count)                 \
95do {                                \
96    int __i;                        \
97    for (__i = (count); __i != 0; __i--) {          \
98        SPINLOCK_BACKOFF_HOOK;              \
99    }                           \
100    if ((count) < SPINLOCK_BACKOFF_MAX)         \
101        (count) += (count);             \
102} while (/* CONSTCOND */ 0);

```


## ZAD 2
1. Pojęcia:
    * Wygłodniała horda (thundering herd) - When a thread releases a resource, it wakes up all threads waiting for it. One of them may now be able to lock the resource; the others will find the resource still locked and will have to go back to sleep. This may lead to extra overhead in wakeups and context switches.
:::spoiler
Even if only one thread was blocked on the resource, there is still a time delay between its
waking up and actually running. In this interval, an unrelated thread may grab the resource, causing
the awakened thread to block again. If this happens frequently, it could lead to starvation of this
thread.
:::
### Czy zwalnianie blokady mutex(9) z użyciem «turnstile_broadcast» nie prowadzi do tego problemu (wygłodniałej hordy)?
Wydaje mi się, że prowadzi.

## ZAD 3
### Czemu jest to mniej wydajne rozwiązanie (wybudzanie pojedynczego) od wybudzenia wszystkich? 
![](https://i.imgur.com/whEs2I3.png)

### Pokaż, że implementacja «mutex_unlock» z użyciem «turnstile_broadcast» unika kosztów związanych z wielokrotnym odpożyczaniem priorytetów.
## ZAD 4
1. Pojęcia:
    * Bariera pamięciowa (memory barrier)

### Jakie dane zawiera struktura muteksa adaptacyjnego w jądrze NetBSD?
### Czemu ich implementacja (MUTEX_ACQUIRE i MUTEX_RELEASE) używa barier pamięciowych (ang. memory barrier )?
Aby procesy czekające na zwolnienie muteksa zaktualizowały pamięć podręczną?
## ZAD 5

[mutex_vector_enter](http://bxr.su/NetBSD/sys/kern/kern_mutex.c#443)
:::spoiler
```C

434/*
435 * mutex_vector_enter:
436 *
437 *  Support routine for mutex_enter() that must handle all cases.  In
438 *  the LOCKDEBUG case, mutex_enter() is always aliased here, even if
439 *  fast-path stubs are available.  If a mutex_spin_enter() stub is
440 *  not available, then it is also aliased directly here.
441 */
442void
443mutex_vector_enter(kmutex_t *mtx)
444{
445    uintptr_t owner, curthread;
446    turnstile_t *ts;
447#ifdef MULTIPROCESSOR
448    u_int count;
449#endif
...
456    /*
457     * Handle spin mutexes.
458     */
459    owner = mtx->mtx_owner;
460    if (MUTEX_SPIN_P(owner)) {
...
464        MUTEX_SPIN_SPLRAISE(mtx);  "SPL - system priority level"
...
466#ifdef FULL
467        if (MUTEX_SPINBIT_LOCK_TRY(mtx)) {
...
469            return;
470        }
471#if !defined(MULTIPROCESSOR)
...
473#else /* !MULTIPROCESSOR */
...
477        count = SPINLOCK_BACKOFF_MIN;
478
479        /*
480         * Spin testing the lock word and do exponential backoff
481         * to reduce cache line ping-ponging between CPUs.
482         */
483        do {
484            while (MUTEX_SPINBIT_LOCKED_P(mtx)) { "sprawdza czy jest zalokowany"
485                SPINLOCK_BACKOFF(count);
...
490            }
491        } while (!MUTEX_SPINBIT_LOCK_TRY(mtx));   "próbuje zalockowac"
...
499#endif  /* !MULTIPROCESSOR */
500#endif  /* FULL */
...
502        return;
503    }
504
505    curthread = (uintptr_t)curlwp;
506
507    MUTEX_DASSERT(mtx, MUTEX_ADAPTIVE_P(owner));
508    MUTEX_ASSERT(mtx, curthread != 0);
509    MUTEX_ASSERT(mtx, !cpu_intr_p());    "?"
...
519    /*
520     * Adaptive mutex; spin trying to acquire the mutex.  If we
521     * determine that the owner is not running on a processor,
522     * then we stop spinning, and sleep instead.
523     */
524    KPREEMPT_DISABLE(curlwp);
525    for (;;) {
526        if (!MUTEX_OWNED(owner)) {
527            /*
528             * Mutex owner clear could mean two things:
529             *
530             *  * The mutex has been released.
531             *  * The owner field hasn't been set yet.
532             *
533             * Try to acquire it again.  If that fails,
534             * we'll just loop again.
535             */
536            if (MUTEX_ACQUIRE(mtx, curthread))
537                break;
538            owner = mtx->mtx_owner;
539            continue;
540        }
541        if (__predict_false(MUTEX_OWNER(owner) == curthread)) {
542            MUTEX_ABORT(mtx, "locking against myself");
543        }
544#ifdef MULTIPROCESSOR
545        /*
546         * Check to see if the owner is running on a processor.
547         * If so, then we should just spin, as the owner will
548         * likely release the lock very soon.
549         */
550        if (mutex_oncpu(owner)) {
...
552            count = SPINLOCK_BACKOFF_MIN;
553            do {
554                KPREEMPT_ENABLE(curlwp);
555                SPINLOCK_BACKOFF(count);
556                KPREEMPT_DISABLE(curlwp);
557                owner = mtx->mtx_owner;
558            } while (mutex_oncpu(owner));
...
561            if (!MUTEX_OWNED(owner))
562                continue;
563        }
564#endif
565
566        ts = turnstile_lookup(mtx);
567
568        /*
569         * Once we have the turnstile chain interlock, mark the
570         * mutex as having waiters.  If that fails, spin again:
571         * chances are that the mutex has been released.
572         */
573        if (!MUTEX_SET_WAITERS(mtx, owner)) {
574            turnstile_exit(mtx);
575            owner = mtx->mtx_owner;
576            continue;
577        }
578
579#ifdef MULTIPROCESSOR
580        /*
581         * mutex_exit() is permitted to release the mutex without
582         * any interlocking instructions, and the following can
583         * occur as a result:
584         *
585         *  CPU 1: MUTEX_SET_WAITERS()      CPU2: mutex_exit()
586         * ---------------------------- ----------------------------
587         *      ..          acquire cache line
588         *      ..                   test for waiters
589         *  acquire cache line    <-      lose cache line
590         *   lock cache line               ..
591         *     verify mutex is held                ..
592         *      set waiters                ..
593         *   unlock cache line         ..
594         *    lose cache line     ->    acquire cache line
595         *      ..            clear lock word, waiters
596         *    return success
597         *
598         * There is another race that can occur: a third CPU could
599         * acquire the mutex as soon as it is released.  Since
600         * adaptive mutexes are primarily spin mutexes, this is not
601         * something that we need to worry about too much.  What we
602         * do need to ensure is that the waiters bit gets set.
603         *
604         * To allow the unlocked release, we need to make some
605         * assumptions here:
606         *
607         * o Release is the only non-atomic/unlocked operation
608         *   that can be performed on the mutex.  (It must still
609         *   be atomic on the local CPU, e.g. in case interrupted
610         *   or preempted).
611         *
612         * o At any given time, MUTEX_SET_WAITERS() can only ever
613         *   be in progress on one CPU in the system - guaranteed
614         *   by the turnstile chain lock.
615         *
616         * o No other operations other than MUTEX_SET_WAITERS()
617         *   and release can modify a mutex with a non-zero
618         *   owner field.
619         *
620         * o The result of a successful MUTEX_SET_WAITERS() call
621         *   is an unbuffered write that is immediately visible
622         *   to all other processors in the system.
623         *
624         * o If the holding LWP switches away, it posts a store
625         *   fence before changing curlwp, ensuring that any
626         *   overwrite of the mutex waiters flag by mutex_exit()
627         *   completes before the modification of curlwp becomes
628         *   visible to this CPU.
629         *
630         * o mi_switch() posts a store fence before setting curlwp
631         *   and before resuming execution of an LWP.
632         *
633         * o _kernel_lock() posts a store fence before setting
634         *   curcpu()->ci_biglock_wanted, and after clearing it.
635         *   This ensures that any overwrite of the mutex waiters
636         *   flag by mutex_exit() completes before the modification
637         *   of ci_biglock_wanted becomes visible.
638         *
639         * We now post a read memory barrier (after setting the
640         * waiters field) and check the lock holder's status again.
641         * Some of the possible outcomes (not an exhaustive list):
642         *
643         * 1. The on-CPU check returns true: the holding LWP is
644         *    running again.  The lock may be released soon and
645         *    we should spin.  Importantly, we can't trust the
646         *    value of the waiters flag.
647         *
648         * 2. The on-CPU check returns false: the holding LWP is
649         *    not running.  We now have the opportunity to check
650         *    if mutex_exit() has blatted the modifications made
651         *    by MUTEX_SET_WAITERS().
652         *
653         * 3. The on-CPU check returns false: the holding LWP may
654         *    or may not be running.  It has context switched at
655         *    some point during our check.  Again, we have the
656         *    chance to see if the waiters bit is still set or
657         *    has been overwritten.
658         *
659         * 4. The on-CPU check returns false: the holding LWP is
660         *    running on a CPU, but wants the big lock.  It's OK
661         *    to check the waiters field in this case.
662         *
663         * 5. The has-waiters check fails: the mutex has been
664         *    released, the waiters flag cleared and another LWP
665         *    now owns the mutex.
666         *
667         * 6. The has-waiters check fails: the mutex has been
668         *    released.
669         *
670         * If the waiters bit is not set it's unsafe to go asleep,
671         * as we might never be awoken.
672         */
673        membar_consumer();
674        if (mutex_oncpu(owner)) {
675            turnstile_exit(mtx);
676            owner = mtx->mtx_owner;
677            continue;
678        }
679        membar_consumer();
680        if (!MUTEX_HAS_WAITERS(mtx)) {
681            turnstile_exit(mtx);
682            owner = mtx->mtx_owner;
683            continue;
684        }
685#endif  /* MULTIPROCESSOR */
...
689        turnstile_block(ts, TS_WRITER_Q, mtx, &mutex_syncobj);
...
694        owner = mtx->mtx_owner;
695    }
696    KPREEMPT_ENABLE(curlwp);
...
706}
```
:::


[mutex_vector_exit](http://bxr.su/NetBSD/sys/kern/kern_mutex.c#mutex_vector_exit)
:::spoiler
```C
708/*
709 * mutex_vector_exit:
710 *
711 *  Support routine for mutex_exit() that handles all cases.
712 */
713void
714mutex_vector_exit(kmutex_t *mtx)
715{
716    turnstile_t *ts;
717    uintptr_t curthread;
718
719    if (MUTEX_SPIN_P(mtx->mtx_owner)) {
720#ifdef FULL
721        if (__predict_false(!MUTEX_SPINBIT_LOCKED_P(mtx))) {
722            MUTEX_ABORT(mtx, "exiting unheld spin mutex");
723        }
724        MUTEX_UNLOCKED(mtx);
725        MUTEX_SPINBIT_LOCK_UNLOCK(mtx);
726#endif
727        MUTEX_SPIN_SPLRESTORE(mtx);
728        return;
729    }
730
731#ifndef __HAVE_MUTEX_STUBS
732    /*
733     * On some architectures without mutex stubs, we can enter here to
734     * release mutexes before interrupts and whatnot are up and running.
735     * We need this hack to keep them sweet.
736     */
737    if (__predict_false(cold)) {
738        MUTEX_UNLOCKED(mtx);
739        MUTEX_RELEASE(mtx);
740        return;
741    }
742#endif
743
744    curthread = (uintptr_t)curlwp;
...
746    MUTEX_ASSERT(mtx, MUTEX_OWNER(mtx->mtx_owner) == curthread);
747    MUTEX_UNLOCKED(mtx);
748#if !defined(LOCKDEBUG)
749    __USE(curthread);
750#endif
...
769    /*
770     * Get this lock's turnstile.  This gets the interlock on
771     * the sleep queue.  Once we have that, we can clear the
772     * lock.  If there was no turnstile for the lock, there
773     * were no waiters remaining.
774     */
775    ts = turnstile_lookup(mtx);
776
777    if (ts == NULL) {
778        MUTEX_RELEASE(mtx);
779        turnstile_exit(mtx);
780    } else {
781        MUTEX_RELEASE(mtx);
782        turnstile_wakeup(ts, TS_WRITER_Q,
783            TS_WAITERS(ts, TS_WRITER_Q), NULL);
784    }
785}
:::
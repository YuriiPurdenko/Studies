# SJU 13/05/20
## ZAD 3
### Czemu jest to mniej wydajne rozwiązanie (wybudzanie pojedynczego) od wybudzenia wszystkich? 
![](https://i.imgur.com/whEs2I3.png)

## ZAD 4
1. Pojęcia:
    * Bariera pamięciowa (memory barrier)

### Jakie dane zawiera struktura muteksa adaptacyjnego w jądrze NetBSD?
### Czemu ich implementacja (MUTEX_ACQUIRE i MUTEX_RELEASE) używa barier pamięciowych (ang. memory barrier )?
Aby procesy czekające na zwolnienie muteksa zaktualizowały pamięć podręczną?

## ZAD 5

[mutex_vector_enter](http://bxr.su/NetBSD/sys/kern/kern_mutex.c#443)
:::spoiler
```C

434/*
435 * mutex_vector_enter:
436 *
437 *  Support routine for mutex_enter() that must handle all cases.  In
438 *  the LOCKDEBUG case, mutex_enter() is always aliased here, even if
439 *  fast-path stubs are available.  If a mutex_spin_enter() stub is
440 *  not available, then it is also aliased directly here.
441 */
442void
443mutex_vector_enter(kmutex_t *mtx)
444{
445    uintptr_t owner, curthread;
446    turnstile_t *ts;
447#ifdef MULTIPROCESSOR
448    u_int count;
449#endif
...
456    /*
457     * Handle spin mutexes.
458     */
459    owner = mtx->mtx_owner;
460    if (MUTEX_SPIN_P(owner)) {
...
464        MUTEX_SPIN_SPLRAISE(mtx);  "SPL - system priority level"
...
466#ifdef FULL
467        if (MUTEX_SPINBIT_LOCK_TRY(mtx)) {
...
469            return;
470        }
471#if !defined(MULTIPROCESSOR)
...
473#else /* !MULTIPROCESSOR */
...
477        count = SPINLOCK_BACKOFF_MIN;
478
479        /*
480         * Spin testing the lock word and do exponential backoff
481         * to reduce cache line ping-ponging between CPUs.
482         */
483        do {
484            while (MUTEX_SPINBIT_LOCKED_P(mtx)) { "sprawdza czy jest zalokowany"
485                SPINLOCK_BACKOFF(count);
...
490            }
491        } while (!MUTEX_SPINBIT_LOCK_TRY(mtx));   "próbuje zalockowac"
...
499#endif  /* !MULTIPROCESSOR */
500#endif  /* FULL */
...
502        return;
503    }
504
505    curthread = (uintptr_t)curlwp;
506
507    MUTEX_DASSERT(mtx, MUTEX_ADAPTIVE_P(owner));
508    MUTEX_ASSERT(mtx, curthread != 0);
509    MUTEX_ASSERT(mtx, !cpu_intr_p());    "?"
...
519    /*
520     * Adaptive mutex; spin trying to acquire the mutex.  If we
521     * determine that the owner is not running on a processor,
522     * then we stop spinning, and sleep instead.
523     */
524    KPREEMPT_DISABLE(curlwp);
525    for (;;) {
526        if (!MUTEX_OWNED(owner)) {
527            /*
528             * Mutex owner clear could mean two things:
529             *
530             *  * The mutex has been released.
531             *  * The owner field hasn't been set yet.
532             *
533             * Try to acquire it again.  If that fails,
534             * we'll just loop again.
535             */
536            if (MUTEX_ACQUIRE(mtx, curthread))
537                break;
538            owner = mtx->mtx_owner;
539            continue;
540        }
541        if (__predict_false(MUTEX_OWNER(owner) == curthread)) {
542            MUTEX_ABORT(mtx, "locking against myself");
543        }
544#ifdef MULTIPROCESSOR
545        /*
546         * Check to see if the owner is running on a processor.
547         * If so, then we should just spin, as the owner will
548         * likely release the lock very soon.
549         */
550        if (mutex_oncpu(owner)) {
...
552            count = SPINLOCK_BACKOFF_MIN;
553            do {
554                KPREEMPT_ENABLE(curlwp);
555                SPINLOCK_BACKOFF(count);
556                KPREEMPT_DISABLE(curlwp);
557                owner = mtx->mtx_owner;
558            } while (mutex_oncpu(owner));
...
561            if (!MUTEX_OWNED(owner))
562                continue;
563        }
564#endif
565
566        ts = turnstile_lookup(mtx);
567
568        /*
569         * Once we have the turnstile chain interlock, mark the
570         * mutex as having waiters.  If that fails, spin again:
571         * chances are that the mutex has been released.
572         */
573        if (!MUTEX_SET_WAITERS(mtx, owner)) {
574            turnstile_exit(mtx);
575            owner = mtx->mtx_owner;
576            continue;
577        }
578
579#ifdef MULTIPROCESSOR
580        /*
581         * mutex_exit() is permitted to release the mutex without
582         * any interlocking instructions, and the following can
583         * occur as a result:
584         *
585         *  CPU 1: MUTEX_SET_WAITERS()      CPU2: mutex_exit()
586         * ---------------------------- ----------------------------
587         *      ..          acquire cache line
588         *      ..                   test for waiters
589         *  acquire cache line    <-      lose cache line
590         *   lock cache line               ..
591         *     verify mutex is held                ..
592         *      set waiters                ..
593         *   unlock cache line         ..
594         *    lose cache line     ->    acquire cache line
595         *      ..            clear lock word, waiters
596         *    return success
597         *
598         * There is another race that can occur: a third CPU could
599         * acquire the mutex as soon as it is released.  Since
600         * adaptive mutexes are primarily spin mutexes, this is not
601         * something that we need to worry about too much.  What we
602         * do need to ensure is that the waiters bit gets set.
603         *
604         * To allow the unlocked release, we need to make some
605         * assumptions here:
606         *
607         * o Release is the only non-atomic/unlocked operation
608         *   that can be performed on the mutex.  (It must still
609         *   be atomic on the local CPU, e.g. in case interrupted
610         *   or preempted).
611         *
612         * o At any given time, MUTEX_SET_WAITERS() can only ever
613         *   be in progress on one CPU in the system - guaranteed
614         *   by the turnstile chain lock.
615         *
616         * o No other operations other than MUTEX_SET_WAITERS()
617         *   and release can modify a mutex with a non-zero
618         *   owner field.
619         *
620         * o The result of a successful MUTEX_SET_WAITERS() call
621         *   is an unbuffered write that is immediately visible
622         *   to all other processors in the system.
623         *
624         * o If the holding LWP switches away, it posts a store
625         *   fence before changing curlwp, ensuring that any
626         *   overwrite of the mutex waiters flag by mutex_exit()
627         *   completes before the modification of curlwp becomes
628         *   visible to this CPU.
629         *
630         * o mi_switch() posts a store fence before setting curlwp
631         *   and before resuming execution of an LWP.
632         *
633         * o _kernel_lock() posts a store fence before setting
634         *   curcpu()->ci_biglock_wanted, and after clearing it.
635         *   This ensures that any overwrite of the mutex waiters
636         *   flag by mutex_exit() completes before the modification
637         *   of ci_biglock_wanted becomes visible.
638         *
639         * We now post a read memory barrier (after setting the
640         * waiters field) and check the lock holder's status again.
641         * Some of the possible outcomes (not an exhaustive list):
642         *
643         * 1. The on-CPU check returns true: the holding LWP is
644         *    running again.  The lock may be released soon and
645         *    we should spin.  Importantly, we can't trust the
646         *    value of the waiters flag.
647         *
648         * 2. The on-CPU check returns false: the holding LWP is
649         *    not running.  We now have the opportunity to check
650         *    if mutex_exit() has blatted the modifications made
651         *    by MUTEX_SET_WAITERS().
652         *
653         * 3. The on-CPU check returns false: the holding LWP may
654         *    or may not be running.  It has context switched at
655         *    some point during our check.  Again, we have the
656         *    chance to see if the waiters bit is still set or
657         *    has been overwritten.
658         *
659         * 4. The on-CPU check returns false: the holding LWP is
660         *    running on a CPU, but wants the big lock.  It's OK
661         *    to check the waiters field in this case.
662         *
663         * 5. The has-waiters check fails: the mutex has been
664         *    released, the waiters flag cleared and another LWP
665         *    now owns the mutex.
666         *
667         * 6. The has-waiters check fails: the mutex has been
668         *    released.
669         *
670         * If the waiters bit is not set it's unsafe to go asleep,
671         * as we might never be awoken.
672         */
673        membar_consumer();
674        if (mutex_oncpu(owner)) {
675            turnstile_exit(mtx);
676            owner = mtx->mtx_owner;
677            continue;
678        }
679        membar_consumer();
680        if (!MUTEX_HAS_WAITERS(mtx)) {
681            turnstile_exit(mtx);
682            owner = mtx->mtx_owner;
683            continue;
684        }
685#endif  /* MULTIPROCESSOR */
...
689        turnstile_block(ts, TS_WRITER_Q, mtx, &mutex_syncobj);
...
694        owner = mtx->mtx_owner;
695    }
696    KPREEMPT_ENABLE(curlwp);
...
706}
```
:::


[mutex_vector_exit](http://bxr.su/NetBSD/sys/kern/kern_mutex.c#mutex_vector_exit)
:::spoiler
```C
708/*
709 * mutex_vector_exit:
710 *
711 *  Support routine for mutex_exit() that handles all cases.
712 */
713void
714mutex_vector_exit(kmutex_t *mtx)
715{
716    turnstile_t *ts;
717    uintptr_t curthread;
718
719    if (MUTEX_SPIN_P(mtx->mtx_owner)) {
720#ifdef FULL
721        if (__predict_false(!MUTEX_SPINBIT_LOCKED_P(mtx))) {
722            MUTEX_ABORT(mtx, "exiting unheld spin mutex");
723        }
724        MUTEX_UNLOCKED(mtx);
725        MUTEX_SPINBIT_LOCK_UNLOCK(mtx);
726#endif
727        MUTEX_SPIN_SPLRESTORE(mtx);
728        return;
729    }
730
731#ifndef __HAVE_MUTEX_STUBS
732    /*
733     * On some architectures without mutex stubs, we can enter here to
734     * release mutexes before interrupts and whatnot are up and running.
735     * We need this hack to keep them sweet.
736     */
737    if (__predict_false(cold)) {
738        MUTEX_UNLOCKED(mtx);
739        MUTEX_RELEASE(mtx);
740        return;
741    }
742#endif
743
744    curthread = (uintptr_t)curlwp;
...
746    MUTEX_ASSERT(mtx, MUTEX_OWNER(mtx->mtx_owner) == curthread);
747    MUTEX_UNLOCKED(mtx);
748#if !defined(LOCKDEBUG)
749    __USE(curthread);
750#endif
...
769    /*
770     * Get this lock's turnstile.  This gets the interlock on
771     * the sleep queue.  Once we have that, we can clear the
772     * lock.  If there was no turnstile for the lock, there
773     * were no waiters remaining.
774     */
775    ts = turnstile_lookup(mtx);
776
777    if (ts == NULL) {
778        MUTEX_RELEASE(mtx);
779        turnstile_exit(mtx);
780    } else {
781        MUTEX_RELEASE(mtx);
782        turnstile_wakeup(ts, TS_WRITER_Q,
783            TS_WAITERS(ts, TS_WRITER_Q), NULL);
784    }
785}
```
:::

## ZAD 6

### W jakim celu rogatka śledzi właściciela ts_owner blokady?
Aby mógł dziedziczyć priorytet dzięki czemu będzie mógł wcześniej wypuścić blokadę.
### Czemu istnieją dwie listy wątków zablokowanych ts_blocked na danej rogatce?
Aby mieć osobne kolejki dla readerów i writerów.

### Względem jakiego parametru posortowane są listy «ts_blocked» i dlaczego?
Względem priorytetów, jak będziemy wybudzać to pomaga realizować naszą polityke, która faworyzuje priorytety.

### W jakim celu wątek przechowuje listę rogatek td_contested wszystkich posiadanych blokad, o które istnieje współzawodnictwo?
Chodzi o dziedziczenie priorytetu.

### Czy wybudzanie wątków byłyby prostsze, gdyby rogatki były przypisane na stałe do wątków?
Wydaje mi się, że nie bo musielibyśmy pamiętać co do czego należy i do tego dbać o to, żeby znaleźć takie pary i jak w wskazówce jeśli chcielibyśmy wybudzić w wątek, który użyczył swojej rogatki dodatkowo tworzyć strukture od nowa(przepisywać ją).

## ZAD 7

![](https://i.imgur.com/45X0pis.png)
Współdzielimy blokade pomiędzy procesami.


Jak dziala rw_enter()
1. rw_enter_sleep() - próbuje uzyskać blokadem zaznacza odpowiednie pola,
2. jeśli nie jest dostępna to wola turnstile lookup, aby przygotwac sie do uspienia 
3. jeszcze raz sprawdzamy czy sie blokada nie zwolnila i jesli jest zajeta to sie usypiamy

Jak działa rw_exit()
Bezpośrednio oddaje komuś blokade czy to dla pisarza czy grupy czytelników zgodnie z magicznym algorytmem:
rw_exit_wakeup() - testuje proste przypadki? i 'upuszcza' blokade jesli nikt na nia nie czeka
> When waiters are present, the code grabs the turnstile (sleep queue) associated with the lock and saves the pointer to the
kernel thread of the next write waiter that was on the turnstile's sleep queue (if one exists).

Turnstille czytelników jest kolejką FIFO

1. Jeśli pisarz zwalnia blokade i sa czytelnicy o większym bądź równym priorytecie co czekający pisarze to oni dostają blokade i sa budzeni poprzez turnstille_wakeup oraz dziedziczą priorytet po czytelniku zwracającym locka jeśli im się to opłaca
2. Zwlaniający blokade czytelnik zawsze faworyzuje pisarzy




### Czy pisarze i czytelnicy pożyczają sobie nawzajem priorytet?
Z samego faktu istnienia turnstilli mamy propagacje priorytetów. 
Pisarze pożyczają czytelnikom czasami.
> These readers also inherit the priority of the writer that released the lock if the reader thread is of a lower priority (inheritance is done on a per-reader thread basis when more than one thread is being woken up). Lock ownership handoff is a relatively simple operation. For read locks, there is no notion of a lock owner, so it's a matter of setting the hold count in the lock to reflect the number of readers coming off the turnstile, then issuing the wakeup of each reader.

### Czy blokada dopuszcza głodzenie czytelników lub pisarzy?
Tak w przypadku, gdy mamy pisarzy, którzy mają większy priorytet niż czytelnicy.

### Czy po wybudzeniu z oczekiwania na zwolnienie blokady czytelnicy i pisarze mogą wejść do sekcji krytycznej bez wykonywania dodatkowych operacji na blokadzie?
Tak
> The kernel does a direct transfer of ownership of the lock to one or more of the threads waiting for the lock when the lock is released, either to the next writer or to a group of readers if more than one reader is blocking and no writers are blocking.

## ZAD 8
Przy próbach zdobywania bload ustawiamy odpowiednie bity.

[rw_vector_enter](http://bxr.su/NetBSD/sys/kern/kern_rwlock.c#rw_vector_enter)
1. Ustawiwamy odpowiednie flagi w zmiennych w zależności kim jesteśmy
2. Wchodzimy do pętli
    1. Jeśli blokada nie ma właściciela lub nie musimy czekać próbujemy ją zdobyć, jeśli się udało to powiadamiamy o tym innych i wychodzimy z pętli, wpp. kręcimy się dalej
    2. Dopóki blokada działa na innym procesorze to kręcimy się z backoffem mając nadzieję, że uda nam się ją przechwycić
    3. Powyższe rzeczy zawiodły chyba czas iść spać - szukamy naszego turnstile'a
    4. W sumie przed pójściem spać upewnijmy się czy jednak nie możemy zabrać blokady, jeśli istnieje szansa wróć do początku pętli, wpp. idź spać
::: spoiler
```=C
/*
 * Bits in the owner field of the lock that indicate lock state.  If the
 * WRITE_LOCKED bit is clear, then the owner field is actually a count of
 * the number of readers.  The rw_owner field is laid out like so:
 *
 *  N                     5        4        3        2        1        0
 *  +------------------------------------------------------------------+
 *  | owner or read count | nodbug | <free> | wrlock | wrwant |  wait  |
 *  +------------------------------------------------------------------+
 */
 
#define RW_HAS_WAITERS      0x01UL  /* lock has waiters */
#define RW_WRITE_WANTED     0x02UL  /* >= 1 waiter is a writer */
#define RW_WRITE_LOCKED     0x04UL  /* lock is currently write locked */

#define RW_READ_COUNT_SHIFT 5
 define RW_READ_INCR        (1UL << RW_READ_COUNT_SHIFT)
#define RW_THREAD       ((uintptr_t)-RW_READ_INCR)
#define RW_OWNER(rw)        ((rw)->rw_owner & RW_THREAD)
#define RW_COUNT(rw)        ((rw)->rw_owner & RW_THREAD)
#define RW_FLAGS(rw)        ((rw)->rw_owner & ~RW_THREAD)
```
```C
typedef enum krw_t {
    RW_READER = 0,
    RW_WRITER = 1
} krw_t;
```

```C=287
void
rw_vector_enter(krwlock_t *rw, const krw_t op)
{
    uintptr_t owner, incr, need_wait, set_wait, curthread, next;
    turnstile_t *ts;
    int queue;
    lwp_t *l;

    l = curlwp;
    curthread = (uintptr_t)l;

    if (__predict_true(op == RW_READER)) {
        incr = RW_READ_INCR;
        set_wait = RW_HAS_WAITERS;
        need_wait = RW_WRITE_LOCKED | RW_WRITE_WANTED;
        queue = TS_READER_Q;
    } else {
        RW_ASSERT(rw, op == RW_WRITER);
        incr = curthread | RW_WRITE_LOCKED;
        set_wait = RW_HAS_WAITERS | RW_WRITE_WANTED;
        need_wait = RW_WRITE_LOCKED | RW_THREAD;
        queue = TS_WRITER_Q;
    }

    KPREEMPT_DISABLE(curlwp);
    for (owner = rw->rw_owner;;) {
        if ((owner & need_wait) == 0) {
            next = rw_cas(rw, owner, (owner + incr) &
                ~RW_WRITE_WANTED);
            if (__predict_true(next == owner)) {
                /* Got it! */
                RW_MEMBAR_ENTER();
                break;
            }

            owner = next;
            continue;
        }
        if (__predict_false(RW_OWNER(rw) == curthread)) {
            rw_abort(__func__, __LINE__, rw,
                "locking against myself");
        }
        if (rw_oncpu(owner)) {
            u_int count = SPINLOCK_BACKOFF_MIN;
            do {
                KPREEMPT_ENABLE(curlwp);
                SPINLOCK_BACKOFF(count);
                KPREEMPT_DISABLE(curlwp);
                owner = rw->rw_owner;
            } while (rw_oncpu(owner));
            if ((owner & need_wait) == 0)
                continue;
        }

        ts = turnstile_lookup(rw);

        owner = rw->rw_owner;
        if ((owner & need_wait) == 0 || rw_oncpu(owner)) {
            turnstile_exit(rw);
            continue;
        }
        next = rw_cas(rw, owner, owner | set_wait);
        if (__predict_false(next != owner)) {
            turnstile_exit(rw);
            owner = next;
            continue;
        }

        turnstile_block(ts, queue, rw, &rw_syncobj);

        if (op == RW_READER || (rw->rw_owner & RW_THREAD) == curthread)
            break;

        owner = rw->rw_owner;
    }
    KPREEMPT_ENABLE(curlwp);
}
```
:::
[rw_vector_exit](http://bxr.su/NetBSD/sys/kern/kern_rwlock.c#rw_vector_exit)

Działanie rw_vector_exit:
1. Podobnie jak wcześniej sprawdzamy czy jesteśmy pisarzem czy czytelnikiem i ustawiamy odpowiednie rzeczy
2. Pętla
    1. Jeśli jesteśmy pisarzem to czyścimy wskaźnik na nas, jeśli jesteśmy czytelnikiem to zmniejszamy liczbe czytelników
    2. Jeśli początkowe x bitów się wyczyściło i ktoś czeka to nikt nie ma blokady więc wychodzimy z pętli 
    3. Aktualizujemy początkowe x bitów jeśli się nie udało to powtarzamy pętle - ktoś inny zaktualizował
3. Blokada jest pusta i ktoś na nią czeka
4. Jeśli nie ma czytelników czekających lub jesteśmy czytelnikiem to jeśli jest jeden pisarz to jemu przekaż locka wpp niech sie bija o blokade
5. Jeśli są czytelnicy i jesteśmy pisarzem oddaj locka wszystkim czytelnikom
:::spoiler
```C=440
void
rw_vector_exit(krwlock_t *rw)
{
    uintptr_t curthread, owner, decr, newown, next;
    turnstile_t *ts;
    int rcnt, wcnt;
    lwp_t *l;

    l = curlwp;
    curthread = (uintptr_t)l;
    RW_ASSERT(rw, curthread != 0);

    owner = rw->rw_owner;
    if (__predict_false((owner & RW_WRITE_LOCKED) != 0)) {
        RW_ASSERT(rw, RW_OWNER(rw) == curthread);
        decr = curthread | RW_WRITE_LOCKED;
    } else {
        RW_ASSERT(rw, RW_COUNT(rw) != 0);
        decr = RW_READ_INCR;
    }

    RW_MEMBAR_EXIT();
    for (;;) {
        newown = (owner - decr);
        if ((newown & (RW_THREAD | RW_HAS_WAITERS)) == RW_HAS_WAITERS)
            break;
        next = rw_cas(rw, owner, newown);
        if (__predict_true(next == owner))
            return;
        owner = next;
    }

    ts = turnstile_lookup(rw);
    owner = rw->rw_owner;
    RW_ASSERT(rw, ts != NULL);
    RW_ASSERT(rw, (owner & RW_HAS_WAITERS) != 0);

    wcnt = TS_WAITERS(ts, TS_WRITER_Q);
    rcnt = TS_WAITERS(ts, TS_READER_Q);

    if (rcnt == 0 || decr == RW_READ_INCR) {
        RW_ASSERT(rw, wcnt != 0);
        RW_ASSERT(rw, (owner & RW_WRITE_WANTED) != 0);

        if (rcnt != 0) {
            l = TS_FIRST(ts, TS_WRITER_Q);
            newown = (uintptr_t)l | (owner & RW_NODEBUG);
            newown |= RW_WRITE_LOCKED | RW_HAS_WAITERS;
            if (wcnt > 1)
                newown |= RW_WRITE_WANTED;
            rw_swap(rw, owner, newown);
            turnstile_wakeup(ts, TS_WRITER_Q, 1, l);
        } else {
            newown = owner & RW_NODEBUG;
            newown |= RW_WRITE_WANTED;
            rw_swap(rw, owner, newown);
            turnstile_wakeup(ts, TS_WRITER_Q, wcnt, NULL);
        }
    } else {
        RW_ASSERT(rw, rcnt != 0);

        newown = owner & RW_NODEBUG;
        newown += rcnt << RW_READ_COUNT_SHIFT;
        if (wcnt != 0)
            newown |= RW_HAS_WAITERS | RW_WRITE_WANTED;

        rw_swap(rw, owner, newown);
        turnstile_wakeup(ts, TS_READER_Q, rcnt, NULL);
    }
}
```
:::
### Jakie różnice zauważasz w stosunku do algorytmu opisanego w [3, §17.6.1]?
1. Nie ma dziedziczenia piorytetu
2. Nie ma kolejki fifo na pisarzach
3. I jeśli jesteśmy pisarzem to zawsze staramy wybudzić czytelników
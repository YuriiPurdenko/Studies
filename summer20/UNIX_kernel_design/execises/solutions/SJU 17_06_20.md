# SJU 17/06/20

## ZAD 1
### [pv_entry](http://bxr.su/NetBSD/sys/uvm/pmap/vmpagemd.h#pv_entry)
Każdy wpis pv_entry opisuje pojedyńczy adres translacji i zawiera adres wirtualny, wskaźnik na vm_pmap związany z tym adresem wirtualnym oraz wskaźnik na następny pv_entry, który mapuje ten adres fizyczny.

![W FreBSD](https://i.imgur.com/pwAbt1N.png)
Kosztują nas pamięć, ale dzięki nim podczas np. copy-on-write, bez tego musielibyśmy sprawdzić wszystkie działające procesy, a dzięki temu sprawdzamy tylko te, które korzystają z naszej strony
### Na podstawie pliku pmap.c powiedz, które z procedur interfejsu pmap wymagają użycia tej listy?
* pmap_init(void)
* pmap_page_remove(struct vm_page *pg)
* pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
* pmap_clear_modify(struct vm_page *pg)

## ZAD 2
1. Pojęcia:
    * 

## ZAD 3
### Czemu zawartość TLB poszczególnych rdzeni nie jest automatycznie synchronizowana?
Każdy procesor ma własną tablicę TLB, istnieje niewiele maszyn, które hardware'owo synchronizują TLB, ale jest zazwyczaj tak nie jest. Większość nie pozwala innym procesorom unieważniać ich wpisów.

Sytuacje, w których zmiana wpisu w TLB oddziałuje na wiele procesorów:
>• The page is a kernel page.
• The page is shared by multiple processes, each running on a different processor.
• On multithreaded systems, different threads of the same process may be running concurrently on different processors. If one thread modifies a mapping, all threads must see the change.

Problemem jest, że chcemy **naraz** zmienić tablicę stron i unieważnić wpis w TLB. Z pomocą przychodzi **algorytm zestrzeliwania wpisów TLB**.

### Algorytm zestrzeliwania wpisów TLB z systemu Mach
W implementacji pomocne są informacje, które mamy dla każdego procesora:
* Flagę aktywnośći - informująca czy procesor korzysta z tablicy stron akrtualnie. W przypadku kiedy flaga jest wyczyszczona procesor bierze udział w zestrzeliwaniu wpisu/ów i zapewnia, że nie będzie odwoływał się do modyfikowalnych wpisów pmap'a.
* Kolejka żądań unieważniających mapowanie w pewien obszar.
* Zbiór aktywnych pmap'ów (pmap jest chroniony przez spinlock'a i każdy zawiera liste procesorów, na których jest aktualnie wykorzystywany)


1. Inicjatór wyłącza wszystkie przerwania i czyści swoją flagę aktywności
2. Następnie zakłada lock'a na pmap'a i dodaje do kolejki rządań dla każdego procesora, na którym aktywny jest zalockowany pmap.
3. Wysyła przerwanie-międzyprocesorowe do procesorów z listy pmap'a.
4. Procesory, które otrzymują przerwania wyłączają wszystkie przerwania, a następnie informuja o otrzymaniu rządania poprzez wyczyszczenie flagi aktywności i kręcą się czekając na zdjęcie lock'a z pmap'a.
5. Jeśli wszystkie procesory, do których wysłaliśmy przerwania wykonały rządaną czynność wtedy proces inicjujący:
    * "flushuje" swój wpis TLB
    *  zmienia pmap'a i zwalnia lock'a
6. Procesy uzyskujące dostęp do pmap'a, przetwarzają kolejkę rządań i  "flushują" przestarzałe wpisy TLB
7. Pod koniec wszyscy resetują flagi aktywności, włączają przerwania i wracają do pracy.

![](https://i.imgur.com/JyAQfY4.png)

### Synchronizacja i zakleszczenia w  algorytmie

* Wyłącznie przerwań zapobiega dostarczeniu np. przerwania sprzętowego, który mogłoby wyjałowić procesory na długi okres czasu.
* Lock na pmap'ie zapobiega wystartowaniu algorytmu dla tego samego pmap'a w tym samym czasie na dwóch procesorach
* Wyłączenie przerwań przed lock'iem zapobiega zakleszczeniu np. w przypadku dostania przerwania-międzyprocesorowego od innego procesoru, który chciał wykonać ten sam algorytm dla tego samego pmap'a (mógłby w tym samym czasie wysłać przerwanie trzymając lock'a)
* Czyszczenie flagi przed założeniem locka zapobiega zakleszczeniu w przypadku, kiedy dwa procesy chcą wykonać algorytm dla tego samego pmap'a, ponieważ jeden mógłby czekać na lock'a, a drugi na flagę

### Wady i zalet algorytmu
1. Zalety:
    * Trudny problem w miarę prosto rozwiązany
    * Brak założeń na temat sprzętu oprócz dostępności przerwań-międzyprocesorowych

2. Wady:
    * Nie skaluje się dobrze
    * Kosztowny
## ZAD 4


### Z jakich przyczyn producenci procesorów wprowadzili duże strony (ang. superpages, huge pages)?
Zwiększyć working set, problem ze zwiększeniem TLB. 
Limit wielkości TLB spowodowany jest, że chcemy aby jej sprawdzenie zajmowało 1 cykl.
> Since the process address space are virtual, the CPU and the operating system have to remember which page belong to which process, and where it is stored. Obviously, the more pages you have, the more time it takes to find where the memory is mapped. When a process uses 1GB of memory, that's 262144 entries to look up (1GB / 4K). If one Page Table Entry consume 8bytes, that's 2MB (262144 * 8) to look-up.
### Jak jądro FreeBSD automatycznie wypromowuje ciągły obszar stron do superstrony?
### Jakie problemy sprawia zarządzanie dużymi stronami?
Często mogą być niepotrzebnie ściągane marnując zasoby i spowalniając działanie.

Możliwe zalety:
* zmniejszają misse w TLB
Możliwe wady:
* większa liczba kopiowania pamięci
* większe użycie dyskowego I/O
## ZAD 5
:::spoiler [vm_phys_alloc_freelist_pages](http://bxr.su/FreeBSD/sys/vm/vm_phys.c#vm_phys_alloc_freelist_pages)
```c
/*
 * Allocate a contiguous, power of two-sized set of physical pages from the
 * specified free list.  The free list must be specified using one of the
 * manifest constants VM_FREELIST_*.
 *
 * The free page queues must be locked.
 */
vm_page_t
vm_phys_alloc_freelist_pages(int domain, int freelist, int pool, int order)
{
    struct vm_freelist *alt, *fl;
    vm_page_t m;
    int oind, pind, flind;

    KASSERT(domain >= 0 && domain < vm_ndomains,
        ("vm_phys_alloc_freelist_pages: domain %d is out of range",
        domain));
    KASSERT(freelist < VM_NFREELIST,
        ("vm_phys_alloc_freelist_pages: freelist %d is out of range",
        freelist));
    KASSERT(pool < VM_NFREEPOOL,
        ("vm_phys_alloc_freelist_pages: pool %d is out of range", pool));
    KASSERT(order < VM_NFREEORDER,
        ("vm_phys_alloc_freelist_pages: order %d is out of range", order));

    flind = vm_freelist_to_flind[freelist];
    /* Check if freelist is present */
    if (flind < 0)
        return (NULL);

    vm_domain_free_assert_locked(VM_DOMAIN(domain));
    fl = &vm_phys_free_queues[domain][flind][pool][0];
    for (oind = order; oind < VM_NFREEORDER; oind++) {
        m = TAILQ_FIRST(&fl[oind].pl);
        if (m != NULL) {
            vm_freelist_rem(fl, m, oind);
            /* The order [order, oind) queues are empty. */
            vm_phys_split_pages(m, oind, fl, order, 1);
            return (m);
        }
    }
    return (NULL);
}
```
:::
* Bierzemy flind'a?
* Sprawdzamy czy mamy lock'a na ta domene
* Wskaźnik na wolną listę
* Jeśli bierzemy pierwszą wolną stronę
    * usuwamy z listy wolnych strone
    * Zwracamy znalezioną stronę
* Zwracany NULL
* 
:::spoiler  [vm_phys_alloc_freelist_pages](http://bxr.su/FreeBSD/sys/vm/vm_phys.c#vm_phys_free_pages)
```c
/*
 * Free a contiguous, power of two-sized set of physical pages.
 *
 * The free page queues must be locked.
 */
void
vm_phys_free_pages(vm_page_t m, int order)
{
    struct vm_freelist *fl;
    struct vm_phys_seg *seg;
    vm_paddr_t pa;
    vm_page_t m_buddy;

    KASSERT(m->order == VM_NFREEORDER,
        ("vm_phys_free_pages: page %p has unexpected order %d",
        m, m->order));
    KASSERT(m->pool < VM_NFREEPOOL,
        ("vm_phys_free_pages: page %p has unexpected pool %d",
        m, m->pool));
    KASSERT(order < VM_NFREEORDER,
        ("vm_phys_free_pages: order %d is out of range", order));
    seg = &vm_phys_segs[m->segind];
    vm_domain_free_assert_locked(VM_DOMAIN(seg->domain));
    
    if (order < VM_NFREEORDER - 1) {
    pa = VM_PAGE_TO_PHYS(m);
        do {
            pa ^= ((vm_paddr_t)1 << (PAGE_SHIFT + order));
            if (pa < seg->start || pa >= seg->end)
                break;
            m_buddy = &seg->first_page[atop(pa - seg->start)];
            if (m_buddy->order != order)
                break;
            fl = (*seg->free_queues)[m_buddy->pool];
            vm_freelist_rem(fl, m_buddy, order);
            if (m_buddy->pool != m->pool)
                vm_phys_set_pool(m->pool, m_buddy, order);
            order++;
            pa &= ~(((vm_paddr_t)1 << (PAGE_SHIFT + order)) - 1);
            m = &seg->first_page[atop(pa - seg->start)];
        } while (order < VM_NFREEORDER - 1);
    }
    fl = (*seg->free_queues)[m->pool];
    vm_freelist_add(fl, m, order, 1);
    
}
```
:::
* Bierzemy opis segmentu
* Sprawdzamy czy mamy lock'a na ta domene
* Bierzemy wskaźnik na naszą listę wolnych i się do niej dodajemy

### Algorytm bliźniaków
```c
if (order < VM_NFREEORDER - 1) {
    pa = VM_PAGE_TO_PHYS(m);
    do {
        pa ^= ((vm_paddr_t)1 << (PAGE_SHIFT + order));
        if (pa < seg->start || pa >= seg->end)
            break;
        m_buddy = &seg->first_page[atop(pa - seg->start)];
        if (m_buddy->order != order)
            break;
        fl = (*seg->free_queues)[m_buddy->pool];
        vm_freelist_rem(fl, m_buddy, order);
        if (m_buddy->pool != m->pool)
            vm_phys_set_pool(m->pool, m_buddy, order);
        order++;
        pa &= ~(((vm_paddr_t)1 << (PAGE_SHIFT + order)) - 1);
        m = &seg->first_page[atop(pa - seg->start)];
    } while (order < VM_NFREEORDER - 1);
}
```
## ZAD 6
![](https://i.imgur.com/5eoxw7x.png)
Argumenty vmem_xalloc:
* vmem_t *vm - arena pamięci wirtualnej
* const vmem_size_t size0 - rozmiar 
* vmem_addr_t *addrp - wskaźnik w którym zwracamy początek
![](https://i.imgur.com/MeQNpuK.png)
:::spoiler int flags
```c
 /*
 * Flags to memory allocation functions.
 */
#define M_NOWAIT    0x0001      /* do not block */
#define M_WAITOK    0x0002      /* ok to block */
#define M_ZERO      0x0100      /* bzero the allocation */
#define M_NOVM      0x0200      /* don't ask VM for pages */
#define M_USE_RESERVE   0x0400      /* can alloc out of reserve memory */
#define M_NODUMP    0x0800      /* don't dump pages in this allocation */
#define M_FIRSTFIT  0x1000      /* only for vmem, fast fit */
#define M_BESTFIT   0x2000      /* only for vmem, low fragmentation */
#define M_EXEC      0x4000      /* allocate executable space */
#define M_NEXTFIT   0x8000      /* only for vmem, follow cursor */
```
:::


:::spoiler [vmem_xalloc](http://bxr.su/FreeBSD/sys/kern/subr_vmem.c#vmem_xalloc)
```c
int
vmem_xalloc(vmem_t *vm, const vmem_size_t size0, vmem_size_t align,
    const vmem_size_t phase, const vmem_size_t nocross,
    const vmem_addr_t minaddr, const vmem_addr_t maxaddr, int flags,
    vmem_addr_t *addrp)
{
    const vmem_size_t size = vmem_roundup_size(vm, size0);
    struct vmem_freelist *list;
    struct vmem_freelist *first;
    struct vmem_freelist *end;
    bt_t *bt;
    int error;
    int strat;

    flags &= VMEM_FLAGS;
    strat = flags & VMEM_FITMASK;
    MPASS(size0 > 0);
    MPASS(size > 0);
    MPASS((flags & (M_NOWAIT|M_WAITOK)) != (M_NOWAIT|M_WAITOK));
    if ((flags & M_NOWAIT) == 0)
        WITNESS_WARN(WARN_GIANTOK | WARN_SLEEPOK, NULL, "vmem_xalloc");
    MPASS(!VMEM_CROSS_P(phase, phase + size - 1, nocross));

    *addrp = 0;

    end = &vm->vm_freelist[VMEM_MAXORDER];
    /*
     * choose a free block from which we allocate.
     */
    first = bt_freehead_toalloc(vm, size, strat);
    VMEM_LOCK(vm);
    for (;;) {
        /*
         * Scan freelists looking for a tag that satisfies the
         * allocation.  If we're doing BESTFIT we may encounter
         * sizes below the request.  If we're doing FIRSTFIT we
         * inspect only the first element from each list.
         */
        for (list = first; list < end; list++) {
            LIST_FOREACH(bt, list, bt_freelist) {
                if (bt->bt_size >= size) {
                    error = vmem_fit(bt, size, align, phase,
                        nocross, minaddr, maxaddr, addrp);
                    if (error == 0) {
                        vmem_clip(vm, bt, *addrp, size);
                        goto out;
                    }
                }
                /* FIRST skips to the next list. */
                if (strat == M_FIRSTFIT)
                    break;
            }
        }

        /*
         * Retry if the fast algorithm failed.
         */
        if (strat == M_FIRSTFIT) {
            strat = M_BESTFIT;
            first = bt_freehead_toalloc(vm, size, strat);
            continue;
        }

        /*
         * Try a few measures to bring additional resources into the
         * arena.  If all else fails, we will sleep waiting for
         * resources to be freed.
         */
        if (!vmem_try_fetch(vm, size, align, flags)) {
            error = ENOMEM;
            break;
        }
    }
out:
    VMEM_UNLOCK(vm);
    if (error != 0 && (flags & M_NOWAIT) == 0)
        panic("failed to allocate waiting allocation\n");

    return (error);
}
```
:::
* Zaokrąglamy rządany rozmiar.
* Wyciągamy flagi i politykę.
* Zapewniamy, że coś chcemy zaallocowac (size > 0) i nie mamy sprzecznych flag (na raz chcemy i nie chcemy spać)
* Jeśli istniej możliwość czekania na blok stron sprawdzamy czy nie mamy locków, które byśmy przyblokwali.
* Ustawiamy zakres wolnych list.
* Dostajem odpowiedni boundary tag - bt_freehead_toalloc
   * Bierzemy pierwszy bt, w którym może pojawić się block naszego rozmiaru
   * W przypadku FirstFit'a zapewniamy, że każdy blok jest przynajmniej rządanego rozmiaru
* Zakładamy locka i zaczynamy poszukiwania
* Jak znaleźliśmy i wszystko się zgadza to dopasowujemy bt i wychodzimy
* Wpp. 
    * Spróbujmy jeszcze raz, ale tym razem z Bestfitem (we wcześniejszym kubełku mogło coś być, reszta jest pusta więc nas to dużo nie kosztuje)
    * Jeśli znowu się nie uda to próbujemy ściągnąć zasoby i czekamy na nie - vmem_try_fetch
* Zdejmujemy lock'a upewniamy się czy udało i wychodzimy.


:::spoiler [vmem_xfree](http://bxr.su/FreeBSD/sys/kern/subr_vmem.c#vmem_xfree)
```c
void
vmem_xfree(vmem_t *vm, vmem_addr_t addr, vmem_size_t size)
{
    bt_t *bt;
    bt_t *t;

    MPASS(size > 0);

    VMEM_LOCK(vm);
    bt = bt_lookupbusy(vm, addr);
    MPASS(bt != NULL);
    MPASS(bt->bt_start == addr);
    MPASS(bt->bt_size == vmem_roundup_size(vm, size) ||
        bt->bt_size - vmem_roundup_size(vm, size) <= vm->vm_quantum_mask);
    MPASS(bt->bt_type == BT_TYPE_BUSY);
    bt_rembusy(vm, bt);
    bt->bt_type = BT_TYPE_FREE;

    /* coalesce */
    t = TAILQ_NEXT(bt, bt_seglist);
    if (t != NULL && t->bt_type == BT_TYPE_FREE) {
        MPASS(BT_END(bt) < t->bt_start);    /* YYY */
        bt->bt_size += t->bt_size;
        bt_remfree(vm, t);
        bt_remseg(vm, t);
    }
    t = TAILQ_PREV(bt, vmem_seglist, bt_seglist);
    if (t != NULL && t->bt_type == BT_TYPE_FREE) {
        MPASS(BT_END(t) < bt->bt_start);    /* YYY */
        bt->bt_size += t->bt_size;
        bt->bt_start = t->bt_start;
        bt_remfree(vm, t);
        bt_remseg(vm, t);
    }

    if (!vmem_try_release(vm, bt, false)) {
        bt_insfree(vm, bt);
        VMEM_CONDVAR_BROADCAST(vm);
        bt_freetrim(vm, BT_MAXFREE);
    }
}
```
:::
* Zakładamy lock'a
* Szukamy tagu
* Usuwamy go z listy zajętych tagów.
* Następnie sprawdzamy czy poprzednik i następnik nie jest wolny i łączmy tagi.
## ZAD 7
1. Pojęcia:
    * 

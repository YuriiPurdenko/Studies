# SJU 24/06/20

## ZAD 7
### Co przechowujemy w magazynkach? 
W magazynkach przechowujemy tablice M obiektów oraz metadane.
### Czemu każdy procesor posiada zestaw dwóch magazynków? 
Każdy procesor ma własny magazynek, aby zwiększyć ziarnistość blokad oraz lokalność magazynków. Dwa magazynki zapewniają pewien rodzaj lokalności czasowej.
### W jaki sposób system dobiera dynamicznie rozmiar magazynków? 
W zależności do liczby nieudanych referencji do antałka zwiększa się rozmiar, aby zmniejszyć liczbę nieudanych żądań i podobnie na odwrót, aby zmniejszyć wykorzystywaną pamięć.
### Skąd algorytm bierze pamięć, jeśli procesor wystrzela obydwa magazynki?
Przy pomocy vmem'ma żądane jest, aby dostać zadrutowaną pamięć?

## ZAD 8
1. Pojęcia:
    * 

## ZAD 1
:::spoiler [sys_read](http://bxr.su/NetBSD/sys/kern/sys_generic.c#sys_read)
```c
int
sys_read(struct lwp *l, const struct sys_read_args *uap, register_t *retval)
{
    file_t *fp;
    int fd;

    fd = SCARG(uap, fd);

    if ((fp = fd_getfile(fd)) == NULL)
        return (EBADF);

    if ((fp->f_flag & FREAD) == 0) {
        fd_putfile(fd);
        return (EBADF);
    }

    /* dofileread() will unuse the descriptor for us */
    return (dofileread(fd, fp, SCARG(uap, buf), SCARG(uap, nbyte),
        &fp->f_offset, FOF_UPDATE_OFFSET, retval));
}
```
:::
1. Pobierz deskryptor
2. Zdobywamy wskaźnik na strukturę pliku
3. wykonujemy czytanie
:::spoiler [dofileread](http://bxr.su/NetBSD/sys/kern/sys_generic.c#dofileread)
```c
int
dofileread(int fd, struct file *fp, void *buf, size_t nbyte,
    off_t *offset, int flags, register_t *retval)
{
    struct iovec aiov;
    struct uio auio;
    size_t cnt;
    int error;
    lwp_t *l;

    l = curlwp;

    aiov.iov_base = (void *)buf;
    aiov.iov_len = nbyte;
    auio.uio_iov = &aiov;
    auio.uio_iovcnt = 1;
    auio.uio_resid = nbyte;
    auio.uio_rw = UIO_READ;
    auio.uio_vmspace = l->l_proc->p_vmspace;

    /*
     * Reads return ssize_t because -1 is returned on error.  Therefore
     * we must restrict the length to SSIZE_MAX to avoid garbage return
     * values.
     */
    if (auio.uio_resid > SSIZE_MAX) {
        error = EINVAL;
        goto out;
    }

    cnt = auio.uio_resid;
    error = (*fp->f_ops->fo_read)(fp, offset, &auio, fp->f_cred, flags);
    if (error)
        if (auio.uio_resid != cnt && (error == ERESTART ||
            error == EINTR || error == EWOULDBLOCK))
            error = 0;
    cnt -= auio.uio_resid;
    *retval = cnt;
 out:
    fd_putfile(fd);
    return (error);
}
```
:::
1. Odpowiednie ustawienie iovec i uio
2. Sprawdzam czy nie chce czytać za dużo
3. Czytam
4. Jeśli wystąpił błąd
    * Jeśli coś udału się przeczytać i mam flaga błędu to ERESTART, EINTR, EWOULDBLOCK to to nie jest błąd
5. Oblicz ile naprawdę przeczytaliśmy i ustwa wartość
6. Zamykamy otwarty wcześniej fp w fd_getfile
7. Zwracamy wartość

W tym miejscu ustawiamy nasze [fileops'y](http://bxr.su/NetBSD/sys/kern/vfs_vnops.c#127) dla vnode'a.

::: spoiler [vn_read](http://bxr.su/NetBSD/sys/kern/vfs_vnops.c#vn_read)
```c
/*
 * File table vnode read routine.
 */
static int
vn_read(file_t *fp, off_t *offset, struct uio *uio, kauth_cred_t cred,
    int flags)
{
    struct vnode *vp = fp->f_vnode;
    int error, ioflag, fflag;
    size_t count;

    ioflag = IO_ADV_ENCODE(fp->f_advice);
    fflag = fp->f_flag;
    if (fflag & FNONBLOCK)
        ioflag |= IO_NDELAY;
    if ((fflag & (FFSYNC | FRSYNC)) == (FFSYNC | FRSYNC))
        ioflag |= IO_SYNC;
    if (fflag & FALTIO)
        ioflag |= IO_ALTSEMANTICS;
    if (fflag & FDIRECT)
        ioflag |= IO_DIRECT;
    vn_lock(vp, LK_SHARED | LK_RETRY);
    uio->uio_offset = *offset;
    count = uio->uio_resid;
    error = VOP_READ(vp, uio, ioflag, cred);
    if (flags & FOF_UPDATE_OFFSET)
        *offset += count - uio->uio_resid;
    VOP_UNLOCK(vp);
    return (error);
}
```
:::
1. Ustawiamy odpowiednie flagi, w jaki sposób będziemy czytać
2. Zakładamy blokadę współdzieloną między czytelnikami
3. Aktualizujemy offset
4. Zapisujemy w count ile pozostało do końca pliku
5. Wykonujemy read'a
6. Aktualizujemy offset
7. Zwalniamy blokadę i zwracamy wartość

:::spoiler [VOP_READ](http://bxr.su/NetBSD/sys/kern/vnode_if.c#487)
```c
int
VOP_READ(struct vnode *vp,
    struct uio *uio,
    int ioflag,
    kauth_cred_t cred)
{
    int error;
    bool mpsafe;
    struct vop_read_args a;
    struct mount *mp;
    a.a_desc = VDESC(vop_read);
    a.a_vp = vp;
    a.a_uio = uio;
    a.a_ioflag = ioflag;
    a.a_cred = cred;
    error = vop_pre(vp, &mp, &mpsafe, FST_NO);
    if (error)
        return error;
    error = (VCALL(vp, VOFFSET(vop_read), &a));
    vop_post(vp, mp, mpsafe, FST_NO);
    return error;
}
```
:::
## ZAD 2
:::spoiler [sys_wrtie](http://bxr.su/NetBSD/sys/kern/sys_generic.c#sys_write)
```c
int
sys_write(struct lwp *l, const struct sys_write_args *uap, register_t *retval)
{
    file_t *fp;
    int fd;

    fd = SCARG(uap, fd);

    if ((fp = fd_getfile(fd)) == NULL)
        return (EBADF);

    if ((fp->f_flag & FWRITE) == 0) {
        fd_putfile(fd);
        return (EBADF);
    }

    /* dofilewrite() will unuse the descriptor for us */
    return (dofilewrite(fd, fp, SCARG(uap, buf), SCARG(uap, nbyte),
        &fp->f_offset, FOF_UPDATE_OFFSET, retval));
}
```
:::
Ten sam opis jak z read tylko robimy write'a
:::spoiler [do_filewrite](http://bxr.su/NetBSD/sys/kern/sys_generic.c#dofilewrite)
```c
int
dofilewrite(int fd, struct file *fp, const void *buf,
    size_t nbyte, off_t *offset, int flags, register_t *retval)
{
    struct iovec aiov;
    struct uio auio;
    size_t cnt;
    int error;

    aiov.iov_base = __UNCONST(buf);     /* XXXUNCONST kills const */
    aiov.iov_len = nbyte;
    auio.uio_iov = &aiov;
    auio.uio_iovcnt = 1;
    auio.uio_resid = nbyte;
    auio.uio_rw = UIO_WRITE;
    auio.uio_vmspace = curproc->p_vmspace;

    /*
     * Writes return ssize_t because -1 is returned on error.  Therefore
     * we must restrict the length to SSIZE_MAX to avoid garbage return
     * values.
     */
    if (auio.uio_resid > SSIZE_MAX) {
        error = EINVAL;
        goto out;
    }

    cnt = auio.uio_resid;
    error = (*fp->f_ops->fo_write)(fp, offset, &auio, fp->f_cred, flags);
    if (error) {
        if (auio.uio_resid != cnt && (error == ERESTART ||
            error == EINTR || error == EWOULDBLOCK))
            error = 0;
        if (error == EPIPE && !(fp->f_flag & FNOSIGPIPE)) {
            mutex_enter(&proc_lock);
            psignal(curproc, SIGPIPE);
            mutex_exit(&proc_lock);
        }
    }
    cnt -= auio.uio_resid;
    *retval = cnt;
 out:
    fd_putfile(fd);
    return (error);
}
```
:::
Tutaj podobnie

Tutaj ustawiamy [fileops'y](http://bxr.su/FreeBSD/sys/kern/sys_pipe.c#162) dla naszego pipeline'a. 

:::spoiler [pipe_write](http://bxr.su/FreeBSD/sys/kern/sys_pipe.c#1041)
```c
static int
pipe_write(struct file *fp, struct uio *uio, struct ucred *active_cred,
    int flags, struct thread *td)
{
    struct pipe *wpipe, *rpipe;
    ssize_t orig_resid;
    int desiredsize, error;

    rpipe = fp->f_data;
    wpipe = PIPE_PEER(rpipe);
    PIPE_LOCK(rpipe);
    error = pipelock(wpipe, 1);
    if (error) {
        PIPE_UNLOCK(rpipe);
        return (error);
    }
    /*
     * detect loss of pipe read side, issue SIGPIPE if lost.
     */
    if (wpipe->pipe_present != PIPE_ACTIVE ||
        (wpipe->pipe_state & PIPE_EOF)) {
        pipeunlock(wpipe);
        PIPE_UNLOCK(rpipe);
        return (EPIPE);
    }
    ++wpipe->pipe_busy;

    /* Choose a larger size if it's advantageous */
    desiredsize = max(SMALL_PIPE_SIZE, wpipe->pipe_buffer.size);
    while (desiredsize < wpipe->pipe_buffer.cnt + uio->uio_resid) {
        if (piperesizeallowed != 1)
            break;
        if (amountpipekva > maxpipekva / 2)
            break;
        if (desiredsize == BIG_PIPE_SIZE)
            break;
        desiredsize = desiredsize * 2;
    }

    /* Choose a smaller size if we're in a OOM situation */
    if (amountpipekva > (3 * maxpipekva) / 4 &&
        wpipe->pipe_buffer.size > SMALL_PIPE_SIZE &&
        wpipe->pipe_buffer.cnt <= SMALL_PIPE_SIZE &&
        piperesizeallowed == 1)
        desiredsize = SMALL_PIPE_SIZE;

    /* Resize if the above determined that a new size was necessary */
    if (desiredsize != wpipe->pipe_buffer.size &&
        (wpipe->pipe_state & PIPE_DIRECTW) == 0) {
        PIPE_UNLOCK(wpipe);
        pipespace(wpipe, desiredsize);
        PIPE_LOCK(wpipe);
    }
    MPASS(wpipe->pipe_buffer.size != 0);

    pipeunlock(wpipe);

    orig_resid = uio->uio_resid;

    while (uio->uio_resid) {
        int space;

        pipelock(wpipe, 0);
        if (wpipe->pipe_state & PIPE_EOF) {
            pipeunlock(wpipe);
            error = EPIPE;
            break;
        }

        /*
         * Pipe buffered writes cannot be coincidental with
         * direct writes.  We wait until the currently executing
         * direct write is completed before we start filling the
         * pipe buffer.  We break out if a signal occurs or the
         * reader goes away.
         */
        if (wpipe->pipe_map.cnt != 0) {
            if (wpipe->pipe_state & PIPE_WANTR) {
                wpipe->pipe_state &= ~PIPE_WANTR;
                wakeup(wpipe);
            }
            pipeselwakeup(wpipe);
            wpipe->pipe_state |= PIPE_WANTW;
            pipeunlock(wpipe);
            error = msleep(wpipe, PIPE_MTX(rpipe), PRIBIO | PCATCH,
                "pipbww", 0);
            if (error)
                break;
            else
                continue;
        }

        space = wpipe->pipe_buffer.size - wpipe->pipe_buffer.cnt;

        /* Writes of size <= PIPE_BUF must be atomic. */
        if ((space < uio->uio_resid) && (orig_resid <= PIPE_BUF))
            space = 0;

        if (space > 0) {
            int size;   /* Transfer size */
            int segsize;    /* first segment to transfer */

            /*
             * Transfer size is minimum of uio transfer
             * and free space in pipe buffer.
             */
            if (space > uio->uio_resid)
                size = uio->uio_resid;
            else
                size = space;
            /*
             * First segment to transfer is minimum of
             * transfer size and contiguous space in
             * pipe buffer.  If first segment to transfer
             * is less than the transfer size, we've got
             * a wraparound in the buffer.
             */
            segsize = wpipe->pipe_buffer.size -
                wpipe->pipe_buffer.in;
            if (segsize > size)
                segsize = size;

            /* Transfer first segment */

            PIPE_UNLOCK(rpipe);
            error = uiomove(&wpipe->pipe_buffer.buffer[wpipe->pipe_buffer.in],
                    segsize, uio);
            PIPE_LOCK(rpipe);

            if (error == 0 && segsize < size) {
                KASSERT(wpipe->pipe_buffer.in + segsize ==
                    wpipe->pipe_buffer.size,
                    ("Pipe buffer wraparound disappeared"));
                /*
                 * Transfer remaining part now, to
                 * support atomic writes.  Wraparound
                 * happened.
                 */

                PIPE_UNLOCK(rpipe);
                error = uiomove(
                    &wpipe->pipe_buffer.buffer[0],
                    size - segsize, uio);
                PIPE_LOCK(rpipe);
            }
            if (error == 0) {
                wpipe->pipe_buffer.in += size;
                if (wpipe->pipe_buffer.in >=
                    wpipe->pipe_buffer.size) {
                    KASSERT(wpipe->pipe_buffer.in ==
                        size - segsize +
                        wpipe->pipe_buffer.size,
                        ("Expected wraparound bad"));
                    wpipe->pipe_buffer.in = size - segsize;
                }

                wpipe->pipe_buffer.cnt += size;
                KASSERT(wpipe->pipe_buffer.cnt <=
                    wpipe->pipe_buffer.size,
                    ("Pipe buffer overflow"));
            }
            pipeunlock(wpipe);
            if (error != 0)
                break;
        } else {
            /*
             * If the "read-side" has been blocked, wake it up now.
             */
            if (wpipe->pipe_state & PIPE_WANTR) {
                wpipe->pipe_state &= ~PIPE_WANTR;
                wakeup(wpipe);
            }

            /*
             * don't block on non-blocking I/O
             */
            if (fp->f_flag & FNONBLOCK) {
                error = EAGAIN;
                pipeunlock(wpipe);
                break;
            }

            /*
             * We have no more space and have something to offer,
             * wake up select/poll.
             */
            pipeselwakeup(wpipe);

            wpipe->pipe_state |= PIPE_WANTW;
            pipeunlock(wpipe);
            error = msleep(wpipe, PIPE_MTX(rpipe),
                PRIBIO | PCATCH, "pipewr", 0);
            if (error != 0)
                break;
        }
    }

    pipelock(wpipe, 0);
    --wpipe->pipe_busy;

    if ((wpipe->pipe_busy == 0) && (wpipe->pipe_state & PIPE_WANT)) {
        wpipe->pipe_state &= ~(PIPE_WANT | PIPE_WANTR);
        wakeup(wpipe);
    } else if (wpipe->pipe_buffer.cnt > 0) {
        /*
         * If we have put any characters in the buffer, we wake up
         * the reader.
         */
        if (wpipe->pipe_state & PIPE_WANTR) {
            wpipe->pipe_state &= ~PIPE_WANTR;
            wakeup(wpipe);
        }
    }

    /*
     * Don't return EPIPE if any byte was written.
     * EINTR and other interrupts are handled by generic I/O layer.
     * Do not pretend that I/O succeeded for obvious user error
     * like EFAULT.
     */
    if (uio->uio_resid != orig_resid && error == EPIPE)
        error = 0;

    if (error == 0)
        vfs_timestamp(&wpipe->pipe_mtime);

    /*
     * We have something to offer,
     * wake up select/poll.
     */
    if (wpipe->pipe_buffer.cnt)
        pipeselwakeup(wpipe);

    pipeunlock(wpipe);
    PIPE_UNLOCK(rpipe);
    return (error);
}
```
:::
1. Zakładamy lock'a na czytanie i pisanie pipe'a
2. Jeśli nikt nie pisał do pipe'a i i ma pełny bufor to kończymy z EPIPE
3. Dobieramy rozmiar pipe'a
4. Wchodzimy w pętle i dopóki mamy jakieś bajty to działamy
    * Próbujemy założyć lock'a, jeśli się nie uda wychodzimy z pętli
    * Jeśli ktoś zapisuje do pipe'a bez buforowania to chcemy poczekać, aż skończy
    * Obliczamy ile miejsca mamy w buforze ~ space
    * Dla space > 0
        * ustawiamy rozmiar zapisu ~ size
        *  ustawaiamy rozmiar pierwszego segmentu ~ segsize
        *  Zdjemujemy lock'a z rpipe'a i robimy uiomove
        *  zakładamy lock'a na rpipe'a
        *  Jeśli nie było błędu i coś jeszcze zostały to wysyłamy brakującą część ~ uiomove (atomoy zapis)
        * Jeśli nie było błędu to aktualizujemy wartości dla wpipe'a  
    * space <= 0
        * Budzimy czytelnika jeśli jest zablokowany
        * zwracamy błąd w przypadku kkiedy chcemy blokować przy atomowym zapisie (atomowy zapis)
        * budzimy select i poll ~ pipseselwakeup
        * zasypiamy, aż się zrobi miejsce
5. jeśli nie zajmujemy już wpipe'a to budzimy tych, którzy na niego czekali
6. jeśli coś zapisaliśmy to też budzimy
## ZAD 3
```c
struct kevent {
    uintptr_t   ident;      /* identifier for this event */
    uint32_t    filter;     /* filter for event */
    uint32_t    flags;      /* action flags for kqueue */
    uint32_t    fflags;     /* filter flag value */
    int64_t     data;       /* filter data value */
    void        *udata;     /* opaque user data identifier */
};
```
::: spoiler [kqueue_register](http://bxr.su/NetBSD/sys/kern/kern_event.c#kqueue_register)
```c
struct kevent {
    uintptr_t   ident;      /* identifier for this event */
    uint32_t    filter;     /* filter for event */
    uint32_t    flags;      /* action flags for kqueue */
    uint32_t    fflags;     /* filter flag value */
    int64_t     data;       /* filter data value */
    void        *udata;     /* opaque user data identifier */
};


/*
 * Register a given kevent kev onto the kqueue
 */
static int
kqueue_register(struct kqueue *kq, struct kevent *kev)
{
    struct kfilter *kfilter;
    filedesc_t *fdp;
    file_t *fp;
    fdfile_t *ff;
    struct knote *kn, *newkn;
    struct klist *list;
    int error, fd, rv;

    fdp = kq->kq_fdp;
    fp = NULL;
    kn = NULL;
    error = 0;
    fd = 0;

    newkn = kmem_zalloc(sizeof(*newkn), KM_SLEEP);

    rw_enter(&kqueue_filter_lock, RW_READER);
    kfilter = kfilter_byfilter(kev->filter);
    if (kfilter == NULL || kfilter->filtops == NULL) {
        /* filter not found nor implemented */
        rw_exit(&kqueue_filter_lock);
        kmem_free(newkn, sizeof(*newkn));
        return (EINVAL);
    }

    /* search if knote already exists */
    if (kfilter->filtops->f_isfd) {
        /* monitoring a file descriptor */
        /* validate descriptor */
        if (kev->ident > INT_MAX
            || (fp = fd_getfile(fd = kev->ident)) == NULL) {
            rw_exit(&kqueue_filter_lock);
            kmem_free(newkn, sizeof(*newkn));
            return EBADF;
        }
        mutex_enter(&fdp->fd_lock);
        ff = fdp->fd_dt->dt_ff[fd];
        if (ff->ff_refcnt & FR_CLOSING) {
            error = EBADF;
            goto doneunlock;
        }
        if (fd <= fdp->fd_lastkqfile) {
            SLIST_FOREACH(kn, &ff->ff_knlist, kn_link) {
                if (kq == kn->kn_kq &&
                    kev->filter == kn->kn_filter)
                    break;
            }
        }
    }

    /*
     * kn now contains the matching knote, or NULL if no match
     */
    if (kev->flags & EV_ADD) {
        if (kn == NULL) {
            /* create new knote */
            kn = newkn;
            newkn = NULL;
            kn->kn_obj = fp;
            kn->kn_id = kev->ident;
            kn->kn_kq = kq;
            kn->kn_fop = kfilter->filtops;
            kn->kn_kfilter = kfilter;
            kn->kn_sfflags = kev->fflags;
            kn->kn_sdata = kev->data;
            kev->fflags = 0;
            kev->data = 0;
            kn->kn_kevent = *kev;

            KASSERT(kn->kn_fop != NULL);
            /*
             * apply reference count to knote structure, and
             * do not release it at the end of this routine.
             */
            fp = NULL;

            /* knote jest w fd */
            /* Otherwise, knote is on an fd. */
            list = (struct klist *)
                &fdp->fd_dt->dt_ff[kn->kn_id]->ff_knlist;
            if ((int)kn->kn_id > fdp->fd_lastkqfile)
                fdp->fd_lastkqfile = kn->kn_id;
            
            SLIST_INSERT_HEAD(list, kn, kn_link);

            KERNEL_LOCK(1, NULL);       /* XXXSMP */
            error = (*kfilter->filtops->f_attach)(kn);
            KERNEL_UNLOCK_ONE(NULL);    /* XXXSMP */
            
            atomic_inc_uint(&kfilter->refcnt);
        } else {
            /*
             * The user may change some filter values after the
             * initial EV_ADD, but doing so will not reset any
             * filter which have already been triggered.
             */
            kn->kn_sfflags = kev->fflags;
            kn->kn_sdata = kev->data;
            kn->kn_kevent.udata = kev->udata;
        }
        /*
         * We can get here if we are trying to attach
         * an event to a file descriptor that does not
         * support events, and the attach routine is
         * broken and does not return an error.
         */
        KASSERT(kn->kn_fop != NULL);
        KASSERT(kn->kn_fop->f_event != NULL);
        KERNEL_LOCK(1, NULL);           /* XXXSMP */
        rv = (*kn->kn_fop->f_event)(kn, 0);
        KERNEL_UNLOCK_ONE(NULL);        /* XXXSMP */
        if (rv)
            knote_activate(kn);
    } else {
        if (kn == NULL) {
            error = ENOENT;
            goto doneunlock;
        }
        if (kev->flags & EV_DELETE) {
            /* knote_detach() drops fdp->fd_lock */
            knote_detach(kn, fdp, true);
            goto done;
        }
    }

    /* disable knote */
    if ((kev->flags & EV_DISABLE)) {
        mutex_spin_enter(&kq->kq_lock);
        if ((kn->kn_status & KN_DISABLED) == 0)
            kn->kn_status |= KN_DISABLED;
        mutex_spin_exit(&kq->kq_lock);
    }

    /* enable knote */
    if ((kev->flags & EV_ENABLE)) {
        knote_enqueue(kn);
    }
doneunlock:
    mutex_exit(&fdp->fd_lock);
 done:
    rw_exit(&kqueue_filter_lock);
    if (newkn != NULL)
        kmem_free(newkn, sizeof(*newkn));
    if (fp != NULL)
        fd_putfile(fd);
    return (error);
}


```
:::
1. Allokujemy bilecik i zakładamy lock'a
2. Szukamy kfilter
    * Jeśli się nie uda zwarcamy błąd
3. Sprawdzamy czy mamy deskryptor pliku i czy jest okej
4. ?
5. Jeśli już mamy taki bilecik to kn na niego wskazuje
6. Jeśli nasza flaga zawiera EV_ADD
    * kn == NULL (nie mamy jeszce takiego biletu)
        * ustawiamy wszystkie wartości kn
        * bierzemy liste knote'ow ~ list
        * do listy dodajemy knote'a
        * wykonujemy f_attach
    * kn != NULL
        * mamy już ustawiony nasz klucz więc ustawiamy tylko odpowiednie rzeczy
    * wywołaj wydarzenie ~ f_event
7. Jeśli nie dodajemyi nie znaleźliśmy to error
8. Jeśli nie dodajemy i usuwamy to ~ knote_detach
9. Jeśli wyłączamy bilet to ...
10. Jeśli włączamy bilet to ..
11. reszta



:::spoiler [filt_fileattach](http://bxr.su/NetBSD/sys/kern/kern_event.c#filt_fileattach)
```c
static int
filt_fileattach(struct knote *kn)
{
    file_t *fp;

    fp = kn->kn_obj;

    return (*fp->f_ops->fo_kqfilter)(fp, kn);
}
```
:::
1. Po prostu wołamy pipe_kqfilter, które jest [tutaj ustawiane](http://bxr.su/NetBSD/sys/kern/sys_pipe.c#131).

:::spoiler [pipe_kqfilter](http://bxr.su/NetBSD/sys/kern/sys_pipe.c#pipe_kqfilter)
```c
static int
pipe_kqfilter(file_t *fp, struct knote *kn)
{
    struct pipe *pipe;
    kmutex_t *lock;

    pipe = ((file_t *)kn->kn_obj)->f_pipe;
    lock = pipe->pipe_lock;

    mutex_enter(lock);

    // kod dla  EVFILT_READ
    kn->kn_fop = &pipe_rfiltops;

    kn->kn_hook = pipe;
    SLIST_INSERT_HEAD(&pipe->pipe_sel.sel_klist, kn, kn_selnext);
    mutex_exit(lock);

    return (0);
}
```
:::
1. Ustawiamy filtops'y dla READ'a
## ZAD 4
:::spoiler [pipe_read](http://bxr.su/NetBSD/sys/kern/sys_pipe.c#pipe_read)
```c

static int
pipe_read(file_t *fp, off_t *offset, struct uio *uio, kauth_cred_t cred,
    int flags)
{
    struct pipe *rpipe = fp->f_pipe;
    struct pipebuf *bp = &rpipe->pipe_buffer;
    kmutex_t *lock = rpipe->pipe_lock;
    int error;
    size_t nread = 0;
    size_t size;
    size_t ocnt;
    unsigned int wakeup_state = 0;

    mutex_enter(lock);
    ++rpipe->pipe_busy;
    ocnt = bp->cnt;

again:
    error = pipelock(rpipe, true);
    if (error)
        goto unlocked_error;

    while (uio->uio_resid) {
        /*
         * Normal pipe buffer receive.
         */
        if (bp->cnt > 0) {
            size = bp->size - bp->out;
            if (size > bp->cnt)
                size = bp->cnt;
            if (size > uio->uio_resid)
                size = uio->uio_resid;

            mutex_exit(lock);
            error = uiomove((char *)bp->buffer + bp->out, size, uio);
            mutex_enter(lock);
            if (error)
                break;

            bp->out += size;
            if (bp->out >= bp->size)
                bp->out = 0;

            bp->cnt -= size;

            /*
             * If there is no more to read in the pipe, reset
             * its pointers to the beginning.  This improves
             * cache hit stats.
             */
            if (bp->cnt == 0) {
                bp->in = 0;
                bp->out = 0;
            }
            nread += size;
            continue;
        }

        /*
         * Break if some data was read.
         */
        if (nread > 0)
            break;

        /*
         * Detect EOF condition.
         * Read returns 0 on EOF, no need to set error.
         */
        if (rpipe->pipe_state & PIPE_EOF)
            break;

        /*
         * Don't block on non-blocking I/O.
         */
        if (fp->f_flag & FNONBLOCK) {
            error = EAGAIN;
            break;
        }

        /*
         * Unlock the pipe buffer for our remaining processing.
         * We will either break out with an error or we will
         * sleep and relock to loop.
         */
        pipeunlock(rpipe);

        /*
         * Re-check to see if more direct writes are pending.
         */
        if ((rpipe->pipe_state & PIPE_DIRECTR) != 0)
            goto again;

#if 1   /* XXX (dsl) I'm sure these aren't needed here ... */
        /*
         * We want to read more, wake up select/poll.
         */
        pipeselwakeup(rpipe, rpipe->pipe_peer, POLL_OUT);

        /*
         * If the "write-side" is blocked, wake it up now.
         */
        cv_broadcast(&rpipe->pipe_wcv);
#endif

        if (wakeup_state & PIPE_RESTART) {
            error = ERESTART;
            goto unlocked_error;
        }

        /* Now wait until the pipe is filled */
        error = cv_wait_sig(&rpipe->pipe_rcv, lock);
        if (error != 0)
            goto unlocked_error;
        wakeup_state = rpipe->pipe_state;
        goto again;
    }

    if (error == 0)
        getnanotime(&rpipe->pipe_atime);
    pipeunlock(rpipe);

unlocked_error:
    --rpipe->pipe_busy;
    if (rpipe->pipe_busy == 0) {
        rpipe->pipe_state &= ~PIPE_RESTART;
        cv_broadcast(&rpipe->pipe_draincv);
    }
    if (bp->cnt < MINPIPESIZE) {
        cv_broadcast(&rpipe->pipe_wcv);
    }

    /*
     * If anything was read off the buffer, signal to the writer it's
     * possible to write more data. Also send signal if we are here for the
     * first time after last write.
     */
    if ((bp->size - bp->cnt) >= PIPE_BUF
        && (ocnt != bp->cnt || (rpipe->pipe_state & PIPE_SIGNALR))) {
        pipeselwakeup(rpipe, rpipe->pipe_peer, POLL_OUT);
        rpipe->pipe_state &= ~PIPE_SIGNALR;
    }

    mutex_exit(lock);
    return (error);
}
```
:::
1. Ustawiamy wszystko
2. Lockujemy rpipe'a i zaczynamy pętle
3. Dopóki nam coś zostaje uio_resid
    * Jeśli coś mamy w buforze to go opróżniamy i wracamy na początek pętli
    * Jeśli coś już odczytaliśmy to wychodzimy
    * Jeśli natrafiliśmy na EOF to wychodzimy
    * Jeśli nie chcemy blokować to wychodzimy
    * Jeśli jesteśmy "PIIPE_DIRECTR" to przed pętlą lądujemy
    * Obudźmy innych ~ pipeselwakeup
    * Jeśli pisarze są zablokowani obudź ich za pomocą broadcast
    * Obsługa błędu PIPE_RESTART
    * Czekamy aż pipe się zapełni
4. OCZYWISTOŚCI

::: spoiler [pipeselwakeup](http://bxr.su/NetBSD/sys/kern/sys_pipe.c#pipeselwakeup)
```c
/*
 * Select/poll wakup. This also sends SIGIO to peer connected to
 * 'sigpipe' side of pipe.
 */
static void
pipeselwakeup(struct pipe *selp, struct pipe *sigp, int code)
{
    int band;

    switch (code) {
    case POLL_IN:
        band = POLLIN|POLLRDNORM;
        break;
    case POLL_OUT:
        band = POLLOUT|POLLWRNORM;
        break;
    case POLL_HUP:
        band = POLLHUP;
        break;
    case POLL_ERR:
        band = POLLERR;
        break;
    default:
        band = 0;
        break;
    }

    selnotify(&selp->pipe_sel, band, NOTE_SUBMIT);

    if (sigp == NULL || (sigp->pipe_state & PIPE_ASYNC) == 0)
        return;

    fownsignal(sigp->pipe_pgid, SIGIO, code, band, selp);
}
```
:::
1. Za pomocą switcha ustawiamy odpowieni event
2. Wołamy selnotify
3. Ewentualnie fownsignal - wysyłanie sygnału